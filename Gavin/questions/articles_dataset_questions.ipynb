{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Questions\n",
    "Hey Russ,\n",
    "\n",
    "I've been playing with the articles dataset and am confused by a couple things. Primarily, there are articles in the dataset that\n",
    "1. Are duplicated.\n",
    "2. Have the `article_id` set to something that doesn't make sense.\n",
    "\n",
    "I know this dataset was cleaned by you somewhere else to make the concept-article network CSV, so this might not be particularly important, but I figured I'd send it to you and see if you had any idea what was going on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7l/ffzd6jqd4k1g060d9whf4v2c0000gn/T/ipykernel_1382/1729149775.py:9: DtypeWarning: Columns (1,2,3,4,5,6,7,8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  articles_df = pd.read_csv(DATA_PATH+ARTICLE_FILE)\n"
     ]
    }
   ],
   "source": [
    "# load some packages\n",
    "import pandas as pd\n",
    "\n",
    "# configuration\n",
    "DATA_PATH = 'datasets/concept_network/' # your path file\n",
    "ARTICLE_FILE = 'dimensions_2021_09_01_articles_category_for_2l_code_102.csv.gz' # Applied Mathematics articles\n",
    "\n",
    "# load the dataset\n",
    "articles_df = pd.read_csv(DATA_PATH+ARTICLE_FILE)\n",
    "# throws an error when low_memory=False, I think bc of the year column in the three degenerate entries at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Solution - Use a parquet file\n",
    "ARTICLE_FILE = 'dimensions_2021_09_01_articles_category_for_2l_code_102.parquet.gz' # Applied Mathematics articles\n",
    "\n",
    "# load the dataset\n",
    "articles_df = pd.read_parquet(DATA_PATH+ARTICLE_FILE, engine='fastparquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Duplicated Entries\n",
    "It looks like there are three entries that are duplicated a combined 120k times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are 0 duplicated rows'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Check rows are duplicated\n",
    "num_rows = len(articles_df)\n",
    "num_unique_rows = len(articles_df.drop_duplicates())\n",
    "num_nonunique_rows = num_rows - num_unique_rows\n",
    "\n",
    "f'There are {num_nonunique_rows} duplicated rows'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], dtype: int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Find duplicated rows\n",
    "assert articles_df['article_id'].nunique() == num_unique_rows # unique article id iff unique row\n",
    "\n",
    "unique_article_counts = articles_df.groupby(\n",
    "        'article_id' # group by unique rows\n",
    "    ).size() # count the number of unique elements\n",
    "unique_article_counts = unique_article_counts[unique_article_counts != 1] # drop unique ones, only nonunniques are left\n",
    "unique_article_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Count number of dumplicated rows\n",
    "(unique_article_counts-1).sum() # subtract 1 bc row should (prolly) be counted once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially, there are 3 articles that make up and extra 120k rows in the dataframe.\n",
    "\n",
    "I don't 100% know how this affects our analysis (if at all after you provide the cleaned network data file) but, at the very least, seems very memory inefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weird Articles\n",
    "There are 3 (unique) articles where the `article_id` column seems like the first part of the abstract, and is inconsistent with the other articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>year</th>\n",
       "      <th>date</th>\n",
       "      <th>doi</th>\n",
       "      <th>volume</th>\n",
       "      <th>issue</th>\n",
       "      <th>pages</th>\n",
       "      <th>abstract_preferred</th>\n",
       "      <th>journal_title</th>\n",
       "      <th>metrics_times_cited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [article_id, year, date, doi, volume, issue, pages, abstract_preferred, journal_title, metrics_times_cited]\n",
       "Index: []"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## find these weird entries\n",
    "unique_articles_df = articles_df.drop_duplicates() # we dont want duplicates anymore\n",
    "\n",
    "unique_articles_df[unique_articles_df['article_id'].str[:4] != 'pub.'] # all real articles start with 'pub.' then have a number. These 3 don't"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, I don't know what impact this would have (if any), but at the very least I assume it means these three articles aren't included in our network, since the abstract, which we parse for topics, is in the `article_id` column and not the `abstract_preferred` column."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
